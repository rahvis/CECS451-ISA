{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Resources:\n",
        "\n",
        "Pseudocode- https://www.cse.unsw.edu.au/~cs9417ml/MLP2/\n",
        "\n",
        "Code Resource: https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
        "\n",
        "More on Backpropagation: https://machinelearninggeek.com/backpropagation-neural-network-using-python/\n",
        "\n"
      ],
      "metadata": {
        "id": "TeW6P9ANEnfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backprop on the Seeds Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp"
      ],
      "metadata": {
        "id": "FQnghYAMc9bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More information on ```seeds_dataset.csv``` at ```http://archive.ics.uci.edu/ml/datasets/seeds```\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "To construct the data, seven geometric parameters of wheat kernels were measured:\n",
        "```\n",
        "1. area A,\n",
        "2. perimeter P,\n",
        "3. compactness C = 4*pi*A/P^2,\n",
        "4. length of kernel,\n",
        "5. width of kernel,\n",
        "6. asymmetry coefficient\n",
        "7. length of kernel groove.\n",
        "```\n",
        "All of these parameters were real-valued continuous"
      ],
      "metadata": {
        "id": "UA9-SgkFx4Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Set Information:\n",
        "\n",
        "The examined group comprised kernels belonging to three different varieties of wheat: ```Kama, Rosa and Canadian```\n",
        ", 70 elements each, randomly selected for\n",
        "the experiment. \n"
      ],
      "metadata": {
        "id": "pIShmmy1CUmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset"
      ],
      "metadata": {
        "id": "nq394yIsdCIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())"
      ],
      "metadata": {
        "id": "omLzExZNdD9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup"
      ],
      "metadata": {
        "id": "t4cLs36ddEGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "\tminmax = list()\n",
        "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "\treturn stats"
      ],
      "metadata": {
        "id": "uG7jOp4mdMkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "\tfor row in dataset:\n",
        "\t\tfor i in range(len(row)-1):\n",
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
      ],
      "metadata": {
        "id": "_jkIwOapdO8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split"
      ],
      "metadata": {
        "id": "LDWGBjZldQVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0"
      ],
      "metadata": {
        "id": "3efzRGrzdSj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores"
      ],
      "metadata": {
        "id": "puHdi1hedVpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an implementation of this in a function named ```activate()```. You can see that the function assumes that the bias is the last weight in the list of weights.\n",
        "\n",
        "```activation = sum(weight_i * input_i) + bias```"
      ],
      "metadata": {
        "id": "WKoFnib21GG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n"
      ],
      "metadata": {
        "id": "1-8pQkREdZTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a neuron is activated, we need to transfer the activation to see what the neuron output actually is.\n",
        "\n",
        "Different transfer functions can be used. Here, we use the sigmoid activation function.\n",
        "\n",
        "We can transfer an activation function using the sigmoid function as follows:\n",
        "\n",
        "```output = 1 / (1 + e^(-activation))```"
      ],
      "metadata": {
        "id": "a65cP8Hj25KF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a function named ```transfer()``` that implements the sigmoid equation."
      ],
      "metadata": {
        "id": "h3r0Fv8v3dHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))"
      ],
      "metadata": {
        "id": "FB5ls-i7daDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a function named ```forward_propagate()``` that implements the forward propagation for a row of data from our dataset with our neural network.\n",
        "\n",
        "You can see that a neuron’s output value is stored in the neuron with the name `output`. You can also see that we collect the outputs for a layer in an array named `new_inputs` that becomes the array inputs and is used as inputs for the following layer.\n",
        "\n",
        "The function returns the outputs from the last layer also called the output layer."
      ],
      "metadata": {
        "id": "Cn_9oKWN32xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs"
      ],
      "metadata": {
        "id": "q6RoCfUGdcZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The backpropagation algorithm is named for the way in which weights are trained:\n",
        "\n",
        "This part is broken down into two sections.\n",
        "```\n",
        "1. Transfer Derivative.\n",
        "2. Error Backpropagation\n",
        "```"
      ],
      "metadata": {
        "id": "D2vwkZWF5fd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an output value from a neuron, we need to calculate it’s slope.\n",
        "\n",
        "We are using the sigmoid transfer function, the derivative of which can be calculated as follows:\n",
        "\n",
        "```derivative = output * (1.0 - output)```\n",
        "\n",
        "```transfer_derivative()``` calculates the slope (which is derivative of the sigmoid function)"
      ],
      "metadata": {
        "id": "SRwOrJfh6SUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)"
      ],
      "metadata": {
        "id": "rZLN3u_adetv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to calculate the error for each output neuron, this will give us our error signal (input) to propagate backwards through the network.\n",
        "\n",
        "The error for a given neuron can be calculated as follows:\n",
        "\n",
        "```error = (output - expected) * transfer_derivative(output)```\n",
        "\n",
        "Where ``` expected``` is the expected output value for the neuron, ```output``` is the output value for the neuron and ```transfer_derivative()``` calculates the slope of the neuron’s output value, as shown above."
      ],
      "metadata": {
        "id": "Uh0nW-VJ6nnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This error calculation is used for neurons in the output layer. The expected value is the class value itself. In the hidden layer, things are a little more complicated.\n",
        "\n",
        "The error signal for a neuron in the hidden layer is calculated as the weighted error of each neuron in the output layer. Think of the error traveling back along the weights of the output layer to the neurons in the hidden layer.\n",
        "\n",
        "The back-propagated error signal is accumulated and then used to determine the error for the neuron in the hidden layer, as follows:\n",
        "\n",
        "```error = (weight_k * error_j) * transfer_derivative(output)```\n",
        "Where ```error_j``` is the error signal from the ```j```th neuron in the output layer, ```weight_k``` is the weight that connects the ```k```th neuron to the current neuron and output is the output for the current neuron.\n",
        "\n",
        "Below is a function named ```backward_propagate_error()``` that implements this procedure."
      ],
      "metadata": {
        "id": "_EvcIzY_7VgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the error signal calculated for each neuron is stored with the name ```delta```"
      ],
      "metadata": {
        "id": "nV6D_IOi73xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(neuron['output'] - expected[j])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "metadata": {
        "id": "fxkAxNVNdhef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] -= l_rate * neuron['delta']"
      ],
      "metadata": {
        "id": "W81kXMi9dke0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\toutputs = forward_propagate(network, row)\n",
        "\t\t\texpected = [0 for i in range(n_outputs)]\n",
        "\t\t\texpected[row[-1]] = 1\n",
        "\t\t\tbackward_propagate_error(network, expected)\n",
        "\t\t\tupdate_weights(network, row, l_rate)"
      ],
      "metadata": {
        "id": "qLpOYTo0dmNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a function named ```initialize_network()``` that creates a new neural network ready for training. It accepts three parameters, the number of inputs, the number of neurons to have in the hidden layer and the number of outputs."
      ],
      "metadata": {
        "id": "PDtcbE9Ty8qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network"
      ],
      "metadata": {
        "id": "HkazbLCfdpEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))\n"
      ],
      "metadata": {
        "id": "PqseAVNAdrI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function named ```back_propagation()``` manages the application of the Backpropagation algorithm, first initializing a network, training it on the training dataset and then using the trained network to make predictions on a test dataset."
      ],
      "metadata": {
        "id": "-sDegUgbCuuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "\tn_inputs = len(train[0]) - 1\n",
        "\tn_outputs = len(set([row[-1] for row in train]))\n",
        "\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(network, row)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)"
      ],
      "metadata": {
        "id": "FAIwEhqBds3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YT-tMUZcJdq",
        "outputId": "66117a34-2350-46ac-98a8-641702c7b0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [90.47619047619048, 92.85714285714286, 97.61904761904762, 92.85714285714286, 92.85714285714286]\n",
            "Mean Accuracy: 93.333%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Test Backprop on Seeds dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'seeds_dataset.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# normalize input variables\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.3\n",
        "n_epoch = 500\n",
        "n_hidden = 5\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ]
    }
  ]
}